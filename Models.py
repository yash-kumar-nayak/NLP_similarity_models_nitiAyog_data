# -*- coding: utf-8 -*-
"""final code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCNBnDWByXGIf72o7BVIlqacVMhZcvvl
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install PyMuPDF
!pip install PyPDF2
!pip install reportlab
!pip install fasttext
!pip install gensim

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec

"""# **Bag of Words**"""

import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(' '.join(filtered_words))  # Join the words back into a single string

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Function to train Bag of Words model
def train_bow_model(corpus):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus)
    return X.toarray()

# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Train Bag of Words models for all files
bow_models1 = [train_bow_model(corpus1_file) for corpus1_file in corpus1_files]
bow_models2 = [train_bow_model(corpus2_file) for corpus2_file in corpus2_files]

# Get Bag of Words embeddings for all files
embeddings1_files = bow_models1  # Embeddings for Act Files
embeddings2_files = bow_models2

similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]


# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=[" Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_BOW.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix BOW", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_BOW.xlsx in Google Drive with two sheets: 'Similarity Matrix BOW' and 'Top 3 Files'.")
#It took 5m28s

"""# **Tf-Idf**"""

# complete code with top 3 names
import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import FastText
import pandas as pd

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(' '.join(filtered_words))  # Join the words back into a single string

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Train TF-IDF model
def train_tfidf_model(corpus):
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(corpus)
    return X.toarray()


# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Train TF-IDF models for all files
tfidf_models1 = [train_tfidf_model(corpus1_file) for corpus1_file in corpus1_files]
tfidf_models2 = [train_tfidf_model(corpus2_file) for corpus2_file in corpus2_files]

# Get embeddings for all files
embeddings1_files = tfidf_models1  # Embeddings for Act Files
embeddings2_files = tfidf_models2

# ... your existing code ...
similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]


# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])





# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=[" Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_TF_IDF.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix TI-IDF", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_TF_IDF.xlsx in Google Drive with two sheets: 'Similarity Matrix TF-IDF' and 'Top 3 Files'.")

"""# **Word2Vec**"""

import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
import pandas as pd

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(filtered_words)

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Train Word2Vec model
def train_word2vec_model(corpus):
    model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)
    return model

# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Train Word2Vec models for all files
word2vec_models1 = [train_word2vec_model(corpus1_file) for corpus1_file in corpus1_files]
word2vec_models2 = [train_word2vec_model(corpus2_file) for corpus2_file in corpus2_files]

# Get embeddings for all files
embeddings1_files = [model.wv.vectors for model in word2vec_models1]  # Embeddings for Act Files
embeddings2_files = [model.wv.vectors for model in word2vec_models2]  # Embeddings for Policy Files

similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]


# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=[" Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_Word2Vec.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix Word2Vec", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_Word2Vec.xlsx in Google Drive with two sheets: 'Similarity Matrix Word2Vec' and 'Top 3 Files'.")

"""# **FastText**"""

import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import FastText
import pandas as pd

# Initialize global variables
corpus1 = []
corpus2 = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(filtered_words)

    return corpus

# Function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Preprocess content and add to corpus
                corpus.append(preprocess_paragraph(content))

    return corpus

# Train FastText model
def train_fasttext_model(corpus):
    model = FastText(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)
    return model

# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1 = load_and_preprocess_folder(folder_path1)
corpus1_flat = [word for sublist in corpus1 for word in sublist]

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2 = load_and_preprocess_folder(folder_path2)
corpus2_flat = [word for sublist in corpus2 for word in sublist]

# Train FastText model on both corpora
fasttext_model1 = train_fasttext_model(corpus1_flat)
fasttext_model2 = train_fasttext_model(corpus2_flat)

# Function to calculate similarity between two sets of embeddings
def calculate_similarity(embeddings1, embeddings2):
    # Calculate the average embeddings
    avg_embedding1 = np.mean(embeddings1, axis=0)
    avg_embedding2 = np.mean(embeddings2, axis=0)

    # Calculate the cosine similarity between the average embeddings
    return cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))[0][0]

# Calculate similarity between all pairs of documents
similarity_matrix = []
for doc1_embeddings in corpus1:
    act_sim_scores = []
    for doc2_embeddings in corpus2:
        sim_score = calculate_similarity(fasttext_model1.wv[doc1_embeddings[0]], fasttext_model2.wv[doc2_embeddings[0]])
        act_sim_scores.append(sim_score)
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]

# Create DataFrame for similarity matrix
df = pd.DataFrame(similarity_matrix, index=act_file_names, columns=policy_file_names)

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names, columns=["Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_fasttext.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix fasttext", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_fasttext.xlsx in Google Drive with two sheets: 'Similarity Matrix fasttext' and 'Top 3 Files'.")

"""# **BERT**"""

!pip install transformers torch

# complete code with top 3 names
import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import FastText
import pandas as pd
from transformers import BertTokenizer, BertModel
import torch

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(filtered_words)

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Initialize BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to get BERT embeddings for a paragraph
def get_bert_embeddings(paragraph):
    max_seq_length = 512
    tokens = tokenizer.tokenize(paragraph)
    embeddings = []

    for i in range(0, len(tokens), max_seq_length - 2):  # Subtract 2 for [CLS] and [SEP]
        chunk = tokens[i:i + max_seq_length - 2]
        chunk = ["[CLS]"] + chunk + ["[SEP]"]
        inputs = tokenizer.convert_tokens_to_ids(chunk)
        inputs = torch.tensor([inputs])
        with torch.no_grad():
            outputs = model(inputs)[0]
        pooled_output = torch.mean(outputs, dim=1).detach().numpy()
        embeddings.append(pooled_output)

    return np.concatenate(embeddings, axis=0)


# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Get BERT embeddings for all files
embeddings1_files = [get_bert_embeddings(' '.join(sum(corpus1_file, []))) for corpus1_file in corpus1_files]  # Embeddings for Act Files
embeddings2_files = [get_bert_embeddings(' '.join(sum(corpus2_file, []))) for corpus2_file in corpus2_files]

similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]

# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=["Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_bert.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix BERT", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_bert.xlsx in Google Drive with two sheets: 'Similarity Matrix BERT' and 'Top 3 Files'.")

"""# **RoBERTa**"""

# complete code with top 3 names
import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import torch
from transformers import RobertaTokenizer, RobertaModel

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(filtered_words)

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Initialize RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

# Function to get RoBERTa embeddings for a paragraph
def get_roberta_embeddings(paragraph):
    max_seq_length = 512
    tokens = tokenizer.tokenize(paragraph)
    embeddings = []

    for i in range(0, len(tokens), max_seq_length - 2):  # Subtract 2 for [CLS] and [SEP]
        chunk = tokens[i:i + max_seq_length - 2]
        inputs = tokenizer.convert_tokens_to_ids(chunk)
        inputs = torch.tensor([inputs])
        with torch.no_grad():
            outputs = model(inputs)[0]
        pooled_output = torch.mean(outputs, dim=1).detach().numpy()
        embeddings.append(pooled_output)

    return np.concatenate(embeddings, axis=0)

# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Get RoBERTa embeddings for all files
embeddings1_files = [get_roberta_embeddings(' '.join(sum(corpus1_file, []))) for corpus1_file in corpus1_files]  # Embeddings for Act Files
embeddings2_files = [get_roberta_embeddings(' '.join(sum(corpus2_file, []))) for corpus2_file in corpus2_files]

similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]

# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=["Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_roberta.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix RoBERTa", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_roberta.xlsx in Google Drive with two sheets: 'Similarity Matrix RoBERTa' and 'Top 3 Files'.")

"""# **ALBERT**"""

# complete code with top 3 names
import os
import numpy as np
from PyPDF2 import PdfReader
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from transformers import AlbertTokenizer, AlbertModel
import torch

# Initialize global variables
corpus1 = []
corpus2_files = []

# Function to preprocess the paragraph
def preprocess_paragraph(paragraph):
    lemmatizer = WordNetLemmatizer()
    sentences = sent_tokenize(paragraph)
    corpus = []

    for sentence in sentences:
        words = word_tokenize(sentence)
        clean_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]
        filtered_words = [word for word in clean_words if word not in stopwords.words('english')]
        if filtered_words:
            corpus.append(filtered_words)

    return corpus

# Common function to load and preprocess files in a folder
def load_and_preprocess_folder(folder_path):
    corpus = []
    valid_extensions = ('.pdf')

    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.lower().endswith(valid_extensions):
                file_path = os.path.join(root, file_name)
                content = ""
                pdf_reader = PdfReader(file_path)

                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"

                # Tokenize content into sentences (lists of words)
                corpus.append(preprocess_paragraph(content))

    return corpus

# Initialize ALBERT tokenizer and model
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertModel.from_pretrained('albert-base-v2')

# Function to get ALBERT embeddings for a paragraph
def get_albert_embeddings(paragraph):
    max_seq_length = 512
    tokens = tokenizer.tokenize(paragraph)
    embeddings = []

    for i in range(0, len(tokens), max_seq_length - 2):  # Subtract 2 for [CLS] and [SEP]
        chunk = tokens[i:i + max_seq_length - 2]
        chunk = ["[CLS]"] + chunk + ["[SEP]"]
        inputs = tokenizer.convert_tokens_to_ids(chunk)
        inputs = torch.tensor([inputs])
        with torch.no_grad():
            outputs = model(inputs)[0]
        pooled_output = torch.mean(outputs, dim=1).detach().numpy()
        embeddings.append(pooled_output)

    return np.concatenate(embeddings, axis=0)

# Load and preprocess data from the first folder (Act Files)
folder_path1 = "/content/drive/MyDrive/acts"
corpus1_files = load_and_preprocess_folder(folder_path1)

# Load and preprocess data from the second folder (Policy Files)
folder_path2 = "/content/drive/MyDrive/Policies"
corpus2_files = load_and_preprocess_folder(folder_path2)

# Get ALBERT embeddings for all files
embeddings1_files = [get_albert_embeddings(' '.join(sum(corpus1_file, []))) for corpus1_file in corpus1_files]  # Embeddings for Act Files
embeddings2_files = [get_albert_embeddings(' '.join(sum(corpus2_file, []))) for corpus2_file in corpus2_files]

similarity_matrix = []
for embeddings1 in embeddings1_files:
    act_sim_scores = []
    for embeddings2 in embeddings2_files:
        # Ensure that the number of columns in embeddings1_2d matches the number of columns in embeddings2_2d
        if embeddings1.shape[1] != embeddings2.shape[1]:
            min_columns = min(embeddings1.shape[1], embeddings2.shape[1])
            embeddings1 = embeddings1[:, :min_columns]
            embeddings2 = embeddings2[:, :min_columns]

        # Calculate the average embeddings
        avg_embedding1 = np.mean(embeddings1, axis=0)
        avg_embedding2 = np.mean(embeddings2, axis=0)

        # Calculate the cosine similarity between the average embeddings
        sim_score = cosine_similarity(avg_embedding1.reshape(1, -1), avg_embedding2.reshape(1, -1))

        act_sim_scores.append(sim_score[0][0])
    similarity_matrix.append(act_sim_scores)

# List of file names (Act Files and Policy Files)
act_file_names = [os.path.basename(file) for file in os.listdir(folder_path1)]
policy_file_names = [os.path.basename(file) for file in os.listdir(folder_path2)]

# Create Excel output
num_columns = len(similarity_matrix[0])
df = pd.DataFrame(similarity_matrix, index=act_file_names[:len(similarity_matrix)], columns=policy_file_names[:num_columns])

# Calculate top 3 similar file names
top_3_file_names = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)
df_top_3_files = pd.DataFrame(top_3_file_names.tolist(), index=act_file_names[:len(similarity_matrix)], columns=["Top 1st Similar File", "Top 2nd Similar File", "Top 3rd Similar File"])

# Write to Excel in Google Drive
file_path = "/content/drive/MyDrive/excel-3/similarity_matrix_albert.xlsx"
with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
    df.to_excel(writer, sheet_name="Similarity Matrix ALBERT", index=True)
    df_top_3_files.to_excel(writer, sheet_name="Top 3 Files", index=True)

print("Excel output saved as similarity_matrix_albert.xlsx in Google Drive with two sheets: 'Similarity Matrix ALBERT' and 'Top 3 Files'.")

"""# visual"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx
import os
import textwrap

def save_and_show_plot(fig, output_path):
    plt.savefig(output_path, dpi=500, bbox_inches='tight')
    plt.close(fig)

    print(f"Visualization saved as {output_path}")

def visualize_similarity_heatmap_and_network(file_path, output_dir=".", output_filename=None):
    # Read Excel file
    xls = pd.ExcelFile(file_path)
    df = pd.read_excel(xls, sheet_name=0, index_col=0)

    # Convert values to numeric
    df = df.apply(pd.to_numeric, errors='coerce')

    # Create a figure with subplots for each policy
    num_policies = len(df.columns)
    fig, axes = plt.subplots(nrows=num_policies, ncols=2, figsize=(20, 8*num_policies))

    # Iterate over each policy
    for i, policy in enumerate(df.columns):
        # Extract top 3 acts with highest scores for the current policy
        top_acts = df[policy].nlargest(3)

        # Create a graph for the current policy
        G = nx.Graph()
        for act, similarity in top_acts.items():
            G.add_edge(act, policy, weight=similarity)

        # Sort nodes by similarity score
        sorted_nodes = sorted(G.nodes(data=True), key=lambda x: max([d['weight'] for _, _, d in G.edges(data=True) if (x[0] in _) or (_ in x[0])], default=0), reverse=True)

        # Plot the network diagram for the current policy
        pos = nx.spring_layout(G, seed=42)
        ax_network = axes[i, 0]

        # Color nodes based on similarity score
        color_map = sns.color_palette("Blues_r", len(sorted_nodes))
        node_colors = [color_map[i] for i in range(len(sorted_nodes))]

        nx.draw_networkx_nodes(G, pos, ax=ax_network, node_color=node_colors, node_size=7000)
        nx.draw_networkx_edges(G, pos, ax=ax_network)
        nx.draw_networkx_labels(G, pos, labels={node: textwrap.fill(node, width=10) for node in G.nodes()}, ax=ax_network, font_size=7)
        ax_network.set_title(f"Policy: {policy}")

        # Plot the heatmap for the current policy
        ax_heatmap = axes[i, 1]
        sns.heatmap(df[[policy]].nlargest(10, columns=[policy]), annot=True, cmap="YlGnBu", ax=ax_heatmap)
        ax_heatmap.set_title("Top 10 Acts with Highest Similarity Scores")

    plt.tight_layout()
    plt.subplots_adjust(top=0.95)

    # Save and show the plot
    if output_filename is None:
        output_filename = os.path.splitext(os.path.basename(file_path))[0] + "-heatmap_&_network.png"
    output_path = os.path.join(output_dir, output_filename)
    save_and_show_plot(plt.gcf(), output_path)

def visualize_all_in_folder(folder_path, output_dir="."):
    os.makedirs(output_dir, exist_ok=True)

    # Iterate over files in the folder
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.xlsx'):
            file_path = os.path.join(folder_path, file_name)
            visualize_similarity_heatmap_and_network(file_path, output_dir)

# Example usage
visualize_all_in_folder("/content/drive/MyDrive/excel-3",  output_dir="/content/drive/MyDrive/visual 3")

# 2min 18 sec

"""# analyis of models"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind

# Define the folder path containing the Excel files
folder_path = '/content/drive/MyDrive/excel-3'

# Define the folder path to save the output image
output_folder_path = '/content/drive/MyDrive/visual 3'
os.makedirs(output_folder_path, exist_ok=True)  # Create the folder if it doesn't exist

# Get a list of all Excel files in the folder
excel_files = [file for file in os.listdir(folder_path) if file.lower().endswith('.xlsx')]

# Initialize a dictionary to store model scores
model_scores = {}
model_details = {}

# Function to clean and process data
def clean_data(df):
    # Remove non-numeric columns
    df = df.select_dtypes(include=['number'])

    # Drop rows and columns that are completely NaN
    df = df.dropna(how='all').dropna(axis=1, how='all')

    # Fill remaining NaN values with the mean of the column
    df = df.apply(lambda x: x.fillna(x.mean()), axis=0)

    return df

# Load similarity scores for each model
for file in excel_files:
    model_name = os.path.splitext(file)[0].lower()  # Extract model name from file name

    try:
        df = pd.read_excel(os.path.join(folder_path, file))

        # Clean data
        df = clean_data(df)

        if df.empty:
            print(f"Warning: {file} contains no valid data after cleaning.")
            continue

        # Calculate descriptive statistics for each model
        avg_score = df.mean().mean()  # Calculate the overall average across all pairs
        std_score = df.std().mean()  # Standard deviation across all pairs
        median_score = df.median().median()  # Median across all pairs

        model_scores[model_name] = avg_score
        model_details[model_name] = {
            'average': avg_score,
            'std': std_score,
            'median': median_score,
            'scores': df.values.flatten()  # Flatten to a 1D array for statistical testing
        }

    except Exception as e:
        print(f"Error processing {file}: {e}")

# Create a figure and axis
fig, ax1 = plt.subplots(figsize=(14, 8))

# Plot bar chart for average similarity scores
sns.barplot(x=list(model_scores.keys()), y=list(model_scores.values()), palette='viridis', ax=ax1)
ax1.set_xlabel('Word Embedding Models')
ax1.set_ylabel('Average Similarity Score', color='b')
ax1.set_title('Comparison of Word Embedding Models')
ax1.set_ylim(0, 1)  # Set y-axis limit (0 to 1)
ax1.grid(axis='y')

# Rotate x-axis labels to prevent overlap
plt.xticks(rotation=45, ha='right')

# Annotate the bars with scores
for model, score in model_scores.items():
    ax1.text(list(model_scores.keys()).index(model), score + 0.02, f'{score:.2f}', ha='center', va='bottom')

# Create a secondary y-axis for the line chart
ax2 = ax1.twinx()

# Plot line chart for average similarity scores
avg_scores = list(model_scores.values())
ax2.plot(list(model_scores.keys()), avg_scores, color='r', marker='o', linestyle='-', linewidth=2, markersize=8)
ax2.set_ylabel('Average Similarity Score', color='r')

# Synchronize the y-axis limits
ax2.set_ylim(ax1.get_ylim())

# Save the figure as a PNG file
output_file_path = os.path.join(output_folder_path, 'model_comparison.png')
plt.tight_layout()  # Adjust layout to make room for rotated x-axis labels
plt.savefig(output_file_path, format='png')

plt.show()

# Print detailed statistics
for model, details in model_details.items():
    print(f"Model: {model}")
    print(f"  Average Score: {details['average']:.2f}")
    print(f"  Standard Deviation: {details['std']:.2f}")
    print(f"  Median Score: {details['median']:.2f}")

# Perform statistical tests (e.g., t-test) to compare models
model_names = list(model_details.keys())
for i in range(len(model_names)):
    for j in range(i + 1, len(model_names)):
        model1 = model_names[i]
        model2 = model_names[j]
        scores1 = model_details[model1]['scores']
        scores2 = model_details[model2]['scores']
        t_stat, p_value = ttest_ind(scores1, scores2, equal_var=False)  # Welch's t-test
        print(f"T-test between {model1} and {model2}:")
        print(f"  T-statistic: {t_stat:.2f}, P-value: {p_value:.4f}")